{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR-bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwd8EdMNxgyM"
      },
      "source": [
        "### Acceso a drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZUUqTBYxXO6",
        "outputId": "33a2a951-cffa-4d50-b33c-0b4cddadc39f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPDzCAKi0lwX"
      },
      "source": [
        "# Bibliotecas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA-T4q-z1CBb",
        "outputId": "2b40746a-29c7-40d2-d0c1-0a23732d7eab"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip3 install pytesseract\n",
        "!cp drive/MyDrive/HackathonRIIAA2021/Data/spa.traineddata /usr/share/tesseract-ocr/4.00/tessdata/\n",
        "\n",
        "!apt install enchant\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install myspell-es"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 2s (2,920 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.8-py2.py3-none-any.whl size=14072 sha256=a9bb46bbf5a6bb89d0829ac2f281c89fe7df0db155c3ed6c4143102ba6566603\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/89/b9/3f11250225d0f90e5454fcc30fd1b7208db226850715aa9ace\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.8\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 1,312 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.2 [310 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.2 [87.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,312 kB in 1s (1,057 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 148533 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  hunspell libreoffice-core | openoffice.org-hunspell | openoffice.org-core\n",
            "  iceape-browser | iceweasel | icedove\n",
            "The following NEW packages will be installed:\n",
            "  myspell-es\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 201 kB of archives.\n",
            "After this operation, 1,004 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 myspell-es all 1.11-14 [201 kB]\n",
            "Fetched 201 kB in 1s (212 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package myspell-es.\n",
            "(Reading database ... 148947 files and directories currently installed.)\n",
            "Preparing to unpack .../myspell-es_1.11-14_all.deb ...\n",
            "Unpacking myspell-es (1.11-14) ...\n",
            "Setting up myspell-es (1.11-14) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX9vtl2Wxe4T"
      },
      "source": [
        "import os \n",
        "import re\n",
        "import string\n",
        "import pytesseract as ocr\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from enchant.checker import SpellChecker\n",
        "from difflib import SequenceMatcher"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ACMzzoUj3s"
      },
      "source": [
        "# cleanup text\n",
        "def get_personslist(text):\n",
        "    personslist=[]\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if isinstance(chunk, nltk.tree.Tree) and chunk.label() == 'PERSON':\n",
        "                personslist.insert(0, (chunk.leaves()[0][0]))\n",
        "    return list(set(personslist))\n",
        "\n",
        "\n",
        "# using enchant.checker.SpellChecker, identify incorrect words\n",
        "def identify_incorrect_words(text):\n",
        "  rep = { '\\n': ' ', '\\\\': ' ', '\\\"': '\"', '-': ' ', '\"': ' \" ', \n",
        "        '\"': ' \" ', '\"': ' \" ', ',':' , ', '.':' . ', '!':' ! ', \n",
        "        '?':' ? ' , '*':' * ', \n",
        "        '(': ' ( ', ')': ' ) ', '=-\\n':''}\n",
        "  rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
        "  pattern = re.compile(\"|\".join(rep.keys()))\n",
        "  text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
        "  # personslist = get_personslist(text)\n",
        "  # print(\"PERSON LIST\")\n",
        "  # ignorewords = personslist + [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*', '`']\n",
        "  ignorewords = [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*']\n",
        "  spell = SpellChecker(\"es_MX\")\n",
        "  words = text.split()\n",
        "  incorrectwords = [w for w in words if not spell.check(w) and w not in ignorewords and len(w) > 1]\n",
        "  # print(\"Incorrect WOrds: \", len(incorrectwords))\n",
        "  # using enchant.checker.SpellChecker, get suggested replacements\n",
        "  suggestedwords = [spell.suggest(w) for w in incorrectwords]\n",
        "  # print(\"Suggested WOrds: \", len(suggestedwords))\n",
        "  # replace incorrect words with [MASK]\n",
        "  for w in incorrectwords:\n",
        "    # print(w)\n",
        "    # print(text)\n",
        "    text = text.replace(\" \" + w + \" \", ' [MASK] ')\n",
        "    # print(text)\n",
        "  return text, suggestedwords"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M56kb_uLgt1f",
        "outputId": "d7e8c490-3be7-4bef-de80-79d43f80b801"
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer,BertForMaskedLM\n",
        "print('Loading BERT tokenizer...')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_OMFrMfk1N"
      },
      "source": [
        "def get_bert_predictions(text, suggestedwords):\n",
        "  # Load, train and predict using pre-trained model\n",
        "  tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "\n",
        "  #Remove punctuations\n",
        "  punctuations = string.punctuation\n",
        "  punctuations = punctuations.replace('[', '')\n",
        "  punctuations = punctuations.replace(']', '')\n",
        "  for c in text:\n",
        "    if c in punctuations:\n",
        "        text = text.replace(c, \"\")\n",
        "  print(\"Text:\", text)\n",
        "  text = '[CLS] ' + text + ' [SEP]'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  print(\"Tokenized Text\", tokenized_text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  MASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']\n",
        "  # Create the segments tensors\n",
        "  segs = [i for i, e in enumerate(tokenized_text) if e == \".\"]\n",
        "  segments_ids=[]\n",
        "  prev=-1\n",
        "  for k, s in enumerate(segs):\n",
        "      segments_ids = segments_ids + [k] * (s-prev)\n",
        "      prev=s\n",
        "  segments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  # prepare Torch inputs \n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  # Load pre-trained model\n",
        "  model = BertForMaskedLM.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "  model.eval()\n",
        "  \n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      predictions = model(tokens_tensor, segments_tensors)\n",
        "  pred_words=[]\n",
        "  for i in range(len(MASKIDS)):\n",
        "      # print(\"Obtener Predicciones i= {}, len = {}, MASKIDS[i]={}, len predictions= {}\".format(i, len(MASKIDS), MASKIDS[i], len(predictions[0][0])))\n",
        "      preds = torch.topk(predictions[0][0][MASKIDS[i]], k=5) \n",
        "      # print(\"tengo las predicciones\")\n",
        "      indices = preds.indices.tolist()\n",
        "      # print(\"tengo los indices\")\n",
        "      # print(indices)\n",
        "      list1 = tokenizer.convert_ids_to_tokens(indices)\n",
        "      # print(\"tengo los tokens\")\n",
        "      # print(len(suggestedwords))\n",
        "\n",
        "      list2 = suggestedwords[i]\n",
        "      # print(\"Sugerencias SpellChecker\", list2)\n",
        "      # print(\"Sugerencias BERT\", list1)\n",
        "      simmax=0\n",
        "      predicted_token=''\n",
        "      for word1 in list1:\n",
        "          for word2 in list2:\n",
        "              # print(\"Word 1:{}, Word 2:{}\".format(word1, word2))\n",
        "              s = SequenceMatcher(None, word1, word2).ratio()\n",
        "              if s is not None and s > simmax:\n",
        "                  simmax = s\n",
        "                  predicted_token = word1\n",
        "      # print(\"predicted token:\", predicted_token)\n",
        "      text = text.replace('[MASK]', predicted_token, 1)\n",
        "      # print(\"REPLACEEEEEEEEEEEEEEEE=================================================\")\n",
        "      # print(text)\n",
        "  return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTsmbaqX5vEn"
      },
      "source": [
        "### Función para procesar imagenes y utilizar Tessearct-OCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_iFIpiUJOrj"
      },
      "source": [
        "def get_processed_images(src_path, processed_path, texts_path, bin_option=True, border_option=False):\n",
        "  source = [f for f in os.listdir(src_path) if os.path.isfile(os.path.join(src_path, f))]\n",
        "  print('\\n[INFO] Se encontraron {} imagenes para procesar.\\n'.format(len(source)))\n",
        "  \n",
        "  for index in tqdm(range(len(source))):\n",
        "    # Lectura y escalado de imagenes #\n",
        "    image_name = source[index].split('.')[0]\n",
        "    image = cv.imread('{}{}'.format(src_path,source[index]))\n",
        "    image = cv.resize(image, None, fx=1.3, fy=1.3, interpolation=cv.INTER_CUBIC)\n",
        "    \n",
        "    \"\"\"\n",
        "    Preprocesamiento de imagenes\n",
        "    \"\"\"\n",
        "    # Eliminación de ruido causado por sombras en las imagenes #\n",
        "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    dilated_image = cv.dilate(gray_image, np.ones((7,7), np.uint8))\n",
        "    blur_image = cv.medianBlur(dilated_image,21)\n",
        "    diff_image = 255 - cv.absdiff(gray_image, blur_image)\n",
        "    norm_image = diff_image.copy()\n",
        "    cv.normalize(diff_image, norm_image, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
        "    thr_img = cv.threshold(norm_image, 230, 0, cv.THRESH_TRUNC)[1]\n",
        "    cv.normalize(thr_img, thr_img, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
        "    bin_image = cv.threshold(thr_img, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)[1]\n",
        "    if border_option == False:\n",
        "      # Lectura del texto de la imagen mediante OCR sin detección de bordes#\n",
        "      config_options = r'--oem 1'\n",
        "      text = ocr.image_to_string(bin_image, lang='spa', config=config_options)\n",
        "      cv.imwrite('{}{}.png'.format(processed_path,image_name), bin_image)\n",
        "      textfile = open('{}{}.txt'.format(texts_path,image_name),'w')\n",
        "      textfile.write(text)\n",
        "      textfile.close()\n",
        "    else:\n",
        "      # Lectura del texto de la imagen mediante OCR con detección de bordes#\n",
        "      bin_image_inv = cv.threshold(thr_img, 0, 255, cv.THRESH_OTSU | cv.THRESH_BINARY_INV)[1]\n",
        "      rect_kernel = cv.getStructuringElement(cv.MORPH_RECT, (90, 90))\n",
        "      dilation = cv.dilate(bin_image_inv, rect_kernel, iterations = 1)\n",
        "    \n",
        "      contours, hierarchy = cv.findContours(dilation, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
        "      if bin_option:\n",
        "        image_copy = bin_image.copy()\n",
        "      else:\n",
        "        image_copy = gray_image.copy()\n",
        "      count = -1\n",
        "      for cnt in contours:\n",
        "        count +=1\n",
        "        x,y,w,h = cv.boundingRect(cnt)\n",
        "        cropped = image_copy[y:y + h, x:x + w]\n",
        "        h,w = cropped.shape\n",
        "        if h>1500 and w>2500:\n",
        "          config_options = r'--oem 1'\n",
        "          text = ocr.image_to_string(cropped, lang='spa', config=config_options)\n",
        "          text, suggestedwords = identify_incorrect_words(text)\n",
        "          text = get_bert_predictions(text, suggestedwords)\n",
        "          if len(text) > 0:\n",
        "            cv.imwrite('{}{}_{}.png'.format(processed_path,image_name,count), cropped)\n",
        "            textfile = open('{}{}.txt'.format(texts_path,image_name),'a')\n",
        "            textfile.write(text)\n",
        "            textfile.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSC1bpCRxfD9"
      },
      "source": [
        "IMAGES_PATH = 'drive/MyDrive/Datos - Hackathon JusticIA/Fichas_auto/'\n",
        "PROCESSED_PATH = 'drive/MyDrive/HackathonRIIAA2021/Processed_images/Fichas_auto_bert/'\n",
        "TEXTS_PATH = 'drive/MyDrive/HackathonRIIAA2021/Texts/Fichas_auto_bert/'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4h0xPLSxfI_",
        "outputId": "98f6b0e4-9149-49c3-c2d0-e19c4296bfb3"
      },
      "source": [
        "get_processed_images(src_path=IMAGES_PATH, processed_path=PROCESSED_PATH, texts_path=TEXTS_PATH)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Se encontraron 1000 imagenes para procesar.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [2:48:32<00:00, 10.11s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}