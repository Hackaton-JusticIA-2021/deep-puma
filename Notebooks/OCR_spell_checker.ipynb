{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR_spell_checker.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwd8EdMNxgyM"
      },
      "source": [
        "### Acceso a drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZUUqTBYxXO6",
        "outputId": "f8dc160e-4cd4-47a0-ad63-bd3c0e03e006"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPDzCAKi0lwX"
      },
      "source": [
        "# Bibliotecas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA-T4q-z1CBb",
        "outputId": "e6ce5c36-2bda-4bf4-bf47-81e60b60ca8b"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip3 install pytesseract\n",
        "!cp drive/MyDrive/HackathonRIIAA2021/Data/spa.traineddata /usr/share/tesseract-ocr/4.00/tessdata/\n",
        "\n",
        "!apt install enchant\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install myspell-es"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "myspell-es is already the newest version (1.11-14).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX9vtl2Wxe4T"
      },
      "source": [
        "import os \n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import distance\n",
        "import pytesseract as ocr\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from enchant.checker import SpellChecker"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVmrvm-UIzl"
      },
      "source": [
        "Función para identificar palabras incorrectas en el texto resultante del Tesseract-OCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ACMzzoUj3s"
      },
      "source": [
        "# cleanup text\n",
        "def get_personslist(text):\n",
        "    personslist=[]\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if isinstance(chunk, nltk.tree.Tree) and chunk.label() == 'PERSON':\n",
        "                personslist.insert(0, (chunk.leaves()[0][0]))\n",
        "    return list(set(personslist))\n",
        "\n",
        "\n",
        "# using enchant.checker.SpellChecker, identify incorrect words\n",
        "def identify_incorrect_words(text):\n",
        "  rep = { '\\n': ' ', '\\\\': ' ', '\\\"': '\"', '-': ' ', '\"': ' \" ', \n",
        "        '\"': ' \" ', '\"': ' \" ', ',':' , ', '.':' . ', '!':' ! ', \n",
        "        '?':' ? ' , '*':' * ', \n",
        "        '(': ' ( ', ')': ' ) ', '=-\\n':''}\n",
        "        \n",
        "  rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
        "  pattern = re.compile(\"|\".join(rep.keys()))\n",
        "  text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
        "  # personslist = get_personslist(text)\n",
        "  # print(\"PERSON LIST\")\n",
        "  # ignorewords = personslist + [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*', '`']\n",
        "  ignorewords = [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*']\n",
        "  spell = SpellChecker(\"es_MX\")\n",
        "  words = text.split()\n",
        "  incorrectwords = [w for w in words if not spell.check(w) and w not in ignorewords and len(w) > 1]\n",
        "\n",
        "  # using enchant.checker.SpellChecker, get suggested replacements\n",
        "  suggestedwords = [{w: spell.suggest(w)} for w in incorrectwords]\n",
        "  for w in incorrectwords:\n",
        "    text = text.replace(w + \" \", ' [MASK] ', 1)\n",
        "  return text, suggestedwords"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDC5bEm3VUQ6"
      },
      "source": [
        "Función para corregir las palabras previamente identificadas como incorrectas.\n",
        "\n",
        "La corrección se realiza encontrando la palabra más similar mediante la distancia Jaccard, de una lista de sugerencias proporcionadas por un dicccionario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7OKVD_klK2q"
      },
      "source": [
        "def get_close_matches(word, word_options):\n",
        "  min_dist = 2\n",
        "  word_match = \"\"\n",
        "  for v in word_options:\n",
        "    temp = distance.jaccard(word, v)\n",
        "    if temp < min_dist:\n",
        "        min_dist = temp\n",
        "        word_match = v\n",
        "    return word_match\n",
        "\n",
        "def set_suggestion(text, suggestedwords):\n",
        "  index_mask = 0\n",
        "  tokens = text.split(\" \")\n",
        "  for token in tokens:\n",
        "    if token == '[MASK]':\n",
        "      word = list(suggestedwords[index_mask].keys())[0]\n",
        "      word_options =  list(suggestedwords[index_mask].values())[0]\n",
        "      word_match = get_close_matches(word, word_options) if len(word_options) else word\n",
        "      text = text.replace('[MASK]', word_match, 1)\n",
        "      index_mask += 1\n",
        "  return text\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTsmbaqX5vEn"
      },
      "source": [
        "### Función para procesar imagenes y utilizar Tessearct-OCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JMCGafZ5ueE"
      },
      "source": [
        "def get_processed_images(src_path, processed_path, texts_path, bin_option=True, border_option=False):\n",
        "  source = [f for f in os.listdir(src_path) if os.path.isfile(os.path.join(src_path, f))]\n",
        "  print('\\n[INFO] Se encontraron {} imagenes para procesar.\\n'.format(len(source)))\n",
        "  \n",
        "  for index in tqdm(range(len(source))):\n",
        "    # Lectura y escalado de imagenes #\n",
        "    image_name = source[index].split('.')[0]\n",
        "    image = cv.imread('{}{}'.format(src_path,source[index]))\n",
        "    image = cv.resize(image, None, fx=1.3, fy=1.3, interpolation=cv.INTER_CUBIC)\n",
        "    \n",
        "    \"\"\"\n",
        "    Preprocesamiento de imagenes\n",
        "    \"\"\"\n",
        "    # Eliminación de ruido causado por sombras en las imagenes #\n",
        "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    dilated_image = cv.dilate(gray_image, np.ones((7,7), np.uint8))\n",
        "    blur_image = cv.medianBlur(dilated_image,21)\n",
        "    diff_image = 255 - cv.absdiff(gray_image, blur_image)\n",
        "    norm_image = diff_image.copy()\n",
        "    cv.normalize(diff_image, norm_image, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
        "    thr_img = cv.threshold(norm_image, 230, 0, cv.THRESH_TRUNC)[1]\n",
        "    cv.normalize(thr_img, thr_img, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
        "    bin_image = cv.threshold(thr_img, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)[1]\n",
        "    if border_option == False:\n",
        "      # Lectura del texto de la imagen mediante OCR sin detección de bordes#\n",
        "      config_options = r'--oem 1'\n",
        "      text = ocr.image_to_string(bin_image, lang='spa', config=config_options)\n",
        "      cv.imwrite('{}{}.png'.format(processed_path,image_name), bin_image)\n",
        "      textfile = open('{}{}.txt'.format(texts_path,image_name),'w')\n",
        "      textfile.write(text)\n",
        "      textfile.close()\n",
        "    else:\n",
        "      # Lectura del texto de la imagen mediante OCR con detección de bordes#\n",
        "      bin_image_inv = cv.threshold(thr_img, 0, 255, cv.THRESH_OTSU | cv.THRESH_BINARY_INV)[1]\n",
        "      rect_kernel = cv.getStructuringElement(cv.MORPH_RECT, (90, 90))\n",
        "      dilation = cv.dilate(bin_image_inv, rect_kernel, iterations = 1)\n",
        "    \n",
        "      contours, hierarchy = cv.findContours(dilation, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
        "      if bin_option:\n",
        "        image_copy = bin_image.copy()\n",
        "      else:\n",
        "        image_copy = gray_image.copy()\n",
        "      count = -1\n",
        "      for cnt in contours:\n",
        "        count +=1\n",
        "        x,y,w,h = cv.boundingRect(cnt)\n",
        "        cropped = image_copy[y:y + h, x:x + w]\n",
        "        h,w = cropped.shape\n",
        "        if h>1500 and w>2500:\n",
        "          config_options = r'--oem 1'\n",
        "          text = ocr.image_to_string(cropped, lang='spa', config=config_options)\n",
        "          text, suggestedwords = identify_incorrect_words(text)\n",
        "          text = set_suggestion(text, suggestedwords)\n",
        "          if len(text) > 0:\n",
        "            cv.imwrite('{}{}_{}.png'.format(processed_path,image_name,count), cropped)\n",
        "            textfile = open('{}{}.txt'.format(texts_path,image_name),'a')\n",
        "            textfile.write(text)\n",
        "            textfile.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSC1bpCRxfD9"
      },
      "source": [
        "IMAGES_PATH = 'drive/MyDrive/Datos - Hackathon JusticIA/Fichas_auto/'\n",
        "PROCESSED_PATH = 'drive/MyDrive/HackathonRIIAA2021/Processed_images/Fichas_auto_dic/'\n",
        "TEXTS_PATH = 'drive/MyDrive/HackathonRIIAA2021/Texts/Fichas_auto_dic/'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4h0xPLSxfI_",
        "outputId": "a3c5824e-1551-4a9f-8b8d-5314272729f4"
      },
      "source": [
        "get_processed_images(src_path=IMAGES_PATH, processed_path=PROCESSED_PATH, texts_path=TEXTS_PATH)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Se encontraron 1000 imagenes para procesar.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [2:59:54<00:00, 10.79s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}