{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR-bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwd8EdMNxgyM"
      },
      "source": [
        "### Acceso a drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZUUqTBYxXO6",
        "outputId": "351a71f5-5440-443a-abdd-f7d00c66cffb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPDzCAKi0lwX"
      },
      "source": [
        "# Bibliotecas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA-T4q-z1CBb",
        "outputId": "72fb42d6-3aa0-4994-c412-f9a51cfbd956"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip3 install pytesseract\n",
        "!cp drive/MyDrive/HackathonRIIAA2021/Data/spa.traineddata /usr/share/tesseract-ocr/4.00/tessdata/\n",
        "\n",
        "!apt install enchant\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install myspell-es"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "myspell-es is already the newest version (1.11-14).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX9vtl2Wxe4T"
      },
      "source": [
        "import os \n",
        "import re\n",
        "import string\n",
        "import pytesseract as ocr\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from enchant.checker import SpellChecker\n",
        "from difflib import SequenceMatcher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ACMzzoUj3s"
      },
      "source": [
        "# cleanup text\n",
        "def get_personslist(text):\n",
        "    personslist=[]\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if isinstance(chunk, nltk.tree.Tree) and chunk.label() == 'PERSON':\n",
        "                personslist.insert(0, (chunk.leaves()[0][0]))\n",
        "    return list(set(personslist))\n",
        "\n",
        "\n",
        "# using enchant.checker.SpellChecker, identify incorrect words\n",
        "def identify_incorrect_words(text):\n",
        "  rep = { '\\n': ' ', '\\\\': ' ', '\\\"': '\"', '-': ' ', '\"': ' \" ', \n",
        "        '\"': ' \" ', '\"': ' \" ', ',':' , ', '.':' . ', '!':' ! ', \n",
        "        '?':' ? ' , '*':' * ', \n",
        "        '(': ' ( ', ')': ' ) ', '=-\\n':''}\n",
        "  rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
        "  pattern = re.compile(\"|\".join(rep.keys()))\n",
        "  text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n",
        "  # personslist = get_personslist(text)\n",
        "  # print(\"PERSON LIST\")\n",
        "  # ignorewords = personslist + [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*', '`']\n",
        "  ignorewords = [\"!\", \",\", \".\", \"\\\"\", \"?\", '(', ')', '*']\n",
        "  spell = SpellChecker(\"es_MX\")\n",
        "  words = text.split()\n",
        "  incorrectwords = [w for w in words if not spell.check(w) and w not in ignorewords and len(w) > 1]\n",
        "  # print(\"Incorrect WOrds: \", len(incorrectwords))\n",
        "  # using enchant.checker.SpellChecker, get suggested replacements\n",
        "  suggestedwords = [spell.suggest(w) for w in incorrectwords]\n",
        "  # print(\"Suggested WOrds: \", len(suggestedwords))\n",
        "  # replace incorrect words with [MASK]\n",
        "  for w in incorrectwords:\n",
        "    # print(w)\n",
        "    # print(text)\n",
        "    text = text.replace(\" \" + w + \" \", ' [MASK] ')\n",
        "    # print(text)\n",
        "  return text, suggestedwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M56kb_uLgt1f",
        "outputId": "24e05215-efae-4b7e-9380-e2c7fb54e1fd"
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer,BertForMaskedLM\n",
        "print('Loading BERT tokenizer...')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_OMFrMfk1N"
      },
      "source": [
        "def get_bert_predictions(text, suggestedwords):\n",
        "  # Load, train and predict using pre-trained model\n",
        "  tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "\n",
        "  #Remove punctuations\n",
        "  punctuations = string.punctuation\n",
        "  punctuations = punctuations.replace('[', '')\n",
        "  punctuations = punctuations.replace(']', '')\n",
        "  for c in text:\n",
        "    if c in punctuations:\n",
        "        text = text.replace(c, \"\")\n",
        "  print(\"Text:\", text)\n",
        "  text = '[CLS] ' + text + ' [SEP]'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  print(\"Tokenized Text\", tokenized_text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  MASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']\n",
        "  # Create the segments tensors\n",
        "  segs = [i for i, e in enumerate(tokenized_text) if e == \".\"]\n",
        "  segments_ids=[]\n",
        "  prev=-1\n",
        "  for k, s in enumerate(segs):\n",
        "      segments_ids = segments_ids + [k] * (s-prev)\n",
        "      prev=s\n",
        "  segments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  # prepare Torch inputs \n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  # Load pre-trained model\n",
        "  model = BertForMaskedLM.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
        "  model.eval()\n",
        "  \n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      predictions = model(tokens_tensor, segments_tensors)\n",
        "  pred_words=[]\n",
        "  for i in range(len(MASKIDS)):\n",
        "      # print(\"Obtener Predicciones i= {}, len = {}, MASKIDS[i]={}, len predictions= {}\".format(i, len(MASKIDS), MASKIDS[i], len(predictions[0][0])))\n",
        "      preds = torch.topk(predictions[0][0][MASKIDS[i]], k=5) \n",
        "      # print(\"tengo las predicciones\")\n",
        "      indices = preds.indices.tolist()\n",
        "      # print(\"tengo los indices\")\n",
        "      # print(indices)\n",
        "      list1 = tokenizer.convert_ids_to_tokens(indices)\n",
        "      # print(\"tengo los tokens\")\n",
        "      # print(len(suggestedwords))\n",
        "\n",
        "      list2 = suggestedwords[i]\n",
        "      print(\"Sugerencias SpellChecker\", list2)\n",
        "      print(\"Sugerencias BERT\", list1)\n",
        "      simmax=0\n",
        "      predicted_token=''\n",
        "      for word1 in list1:\n",
        "          for word2 in list2:\n",
        "              # print(\"Word 1:{}, Word 2:{}\".format(word1, word2))\n",
        "              s = SequenceMatcher(None, word1, word2).ratio()\n",
        "              if s is not None and s > simmax:\n",
        "                  simmax = s\n",
        "                  predicted_token = word1\n",
        "      # print(\"predicted token:\", predicted_token)\n",
        "      text = text.replace('[MASK]', predicted_token, 1)\n",
        "      # print(\"REPLACEEEEEEEEEEEEEEEE=================================================\")\n",
        "      # print(text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTsmbaqX5vEn"
      },
      "source": [
        "### Función para procesar imagenes y utilizar Tessearct-OCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JMCGafZ5ueE"
      },
      "source": [
        "def get_processed_images(src_path, processed_path, texts_path, bin_option=True):\n",
        "  source = [f for f in os.listdir(src_path) if os.path.isfile(os.path.join(src_path, f))]\n",
        "  print('\\n[INFO] Se encontraron {} imagenes para procesar.\\n'.format(len(source)))\n",
        "  # for index in tqdm(range(len(source))):\n",
        "  for index in tqdm(range(1)):\n",
        "    # Lectura y escalado de imagenes #\n",
        "    image_name = source[index].split('.')[0]\n",
        "    image = cv.imread('{}{}'.format(src_path,source[index]))\n",
        "    image = cv.resize(image, None, fx=1.3, fy=1.3, interpolation=cv.INTER_CUBIC)\n",
        "    \"\"\"\n",
        "    Preprocesamiento de imagenes\n",
        "    \"\"\"\n",
        "    # Eliminación de ruido causado por sombras en las imagenes #\n",
        "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    dilated_image = cv.dilate(gray_image, np.ones((7,7), np.uint8))\n",
        "    blur_image = cv.medianBlur(dilated_image,21)\n",
        "    diff_image = 255 - cv.absdiff(gray_image, blur_image)\n",
        "    norm_image = diff_image.copy()\n",
        "    cv.normalize(diff_image, norm_image, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
        "    thr_img = cv.threshold(norm_image, 230, 0, cv.THRESH_TRUNC)[1]\n",
        "    cv.normalize(thr_img, thr_img, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
        "    bin_image = cv.threshold(thr_img, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)[1]\n",
        "    \n",
        "    # detección de texto #\n",
        "    bin_image_inv = cv.threshold(thr_img, 0, 255, cv.THRESH_OTSU | cv.THRESH_BINARY_INV)[1]\n",
        "    rect_kernel = cv.getStructuringElement(cv.MORPH_RECT, (90, 90))\n",
        "    dilation = cv.dilate(bin_image_inv, rect_kernel, iterations = 1)\n",
        "    \n",
        "    contours, hierarchy = cv.findContours(dilation, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
        "    if bin_option:\n",
        "        image_copy = bin_image.copy()\n",
        "    else:\n",
        "        image_copy = gray_image.copy()\n",
        "    count = -1\n",
        "    for cnt in contours:\n",
        "        count +=1\n",
        "        x,y,w,h = cv.boundingRect(cnt)\n",
        "        cropped = image_copy[y:y + h, x:x + w]\n",
        "        h,w = cropped.shape\n",
        "        if h>490 and w>490:\n",
        "            try:\n",
        "              text = ocr.image_to_string(cropped, lang='spa')\n",
        "              print(\"======================Texto OCR:==========================\")\n",
        "              print(text)\n",
        "              print('\\n')\n",
        "              # print(\"======================Texto Masked:========================\")\n",
        "              text, suggestedwords = identify_incorrect_words(text)\n",
        "              # print(text)\n",
        "              text = get_bert_predictions(text, suggestedwords)\n",
        "              print(\"======================Texto BERT======================\")\n",
        "              print(text)\n",
        "              if len(text) > 0:\n",
        "                  cv.imwrite('{}{}_processed_{}.png'.format(processed_path,image_name,count), cropped)\n",
        "                  textfile = open('{}{}_text.txt'.format(texts_path,image_name),'w')\n",
        "                  textfile.write(text)\n",
        "                  textfile.close()\n",
        "            except:\n",
        "              print(e)\n",
        "              continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSC1bpCRxfD9"
      },
      "source": [
        "IMAGES_PATH = 'drive/MyDrive/Datos - Hackathon JusticIA/Fichas_auto/'\n",
        "PROCESSED_PATH = 'drive/MyDrive/HackathonRIIAA2021/Processed_images/Fichas_auto/'\n",
        "TEXTS_PATH = 'drive/MyDrive/HackathonRIIAA2021/Texts/Fichas_auto/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4h0xPLSxfI_",
        "outputId": "60a256d9-9987-4786-b981-8017ebe8585c"
      },
      "source": [
        "get_processed_images(src_path=IMAGES_PATH, processed_path=PROCESSED_PATH, texts_path=TEXTS_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Se encontraron 1000 imagenes para procesar.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================Texto OCR:==========================\n",
            "dgric.y tomendo en consideración el fallo del Jue,\n",
            "que nulificó las elecciones en cuestión,declare -\n",
            "acéfala la actual directiva de la Unión y convoqui\n",
            "a nuevos Comicios..eEn la C.NoC.,Seé hacen comenta=-\n",
            "rios en el sentido de que el descontento correro-\n",
            "es utilizado-en contra del diricente mÁáximo de -\n",
            "esa Central<,para que- éste no llegue al Cong.Ca -\n",
            "1a Unión9y que el Srio.Cral.de la Loco.éode GrOo. o\n",
            "ha manifestado que él,an lo personal,no está teme\n",
            "roso del problema coprero,porque c-en a con le ayl\n",
            "da del Dip.Loc.LIC RUBEN ZUNO ARCE,gXpÍ)_%Q¿%35D96'Z\n",
            "- . 2 lg o\n",
            "Ag to9-967.> Se tiene conocimie nto de que cl d'ia a\n",
            "h de los corrientes, el Dio, Fod,., CuSaR DEL Alloo\n",
            "GEL FUENTES, tieno ponsado salir con destino a .o\n",
            "Acapulcos Groo; con cl objeto de scoguir ilcvando\n",
            "\n",
            "TA EE NAA EA D s - MA c —\n",
            "NA Ae — en o a.\n",
            "— a\n",
            "\f\n",
            "\n",
            "\n",
            "Text: dgric  y [MASK] en consideración el fallo del [MASK]   que [MASK] las elecciones en cuestión  declare   acéfala la actual directiva de la Unión y [MASK] a nuevos Comicios    [MASK] la C  [MASK]    [MASK] hacen comentarios en el sentido de que el descontento [MASK]  es utilizado en contra del [MASK] [MASK] de   esa [MASK]  para que  éste no llegue al [MASK]  Ca   [MASK] [MASK] que el [MASK]  [MASK]  de la Loco  [MASK] [MASK]   o ha manifestado que él  [MASK] lo personal  no está teme roso del problema [MASK]  porque c en a con le [MASK] da del [MASK]  [MASK]  [MASK] [MASK] [MASK] ARCE  [MASK]  [MASK]      2 [MASK] o [MASK] [MASK] 967   Se tiene [MASK] [MASK] de que [MASK] [MASK] a h de los corrientes   el Dio   [MASK]       [MASK] DEL [MASK] GEL FUENTES   [MASK] [MASK] salir con destino a   o [MASK] [MASK] con [MASK] objeto de [MASK] [MASK]  TA [MASK] [MASK] EA D s   MA c — NA [MASK] — en o a   — a \f\n",
            "Tokenized Text ['[CLS]', 'dg', '##ric', 'y', '[MASK]', 'en', 'consideración', 'el', 'fallo', 'del', '[MASK]', 'que', '[MASK]', 'las', 'elecciones', 'en', 'cuestión', 'declare', 'ac', '##éf', '##ala', 'la', 'actual', 'directiva', 'de', 'la', 'unión', 'y', '[MASK]', 'a', 'nuevos', 'comi', '##cios', '[MASK]', 'la', 'c', '[MASK]', '[MASK]', 'hacen', 'comentarios', 'en', 'el', 'sentido', 'de', 'que', 'el', 'descontento', '[MASK]', 'es', 'utilizado', 'en', 'contra', 'del', '[MASK]', '[MASK]', 'de', 'esa', '[MASK]', 'para', 'que', 'éste', 'no', 'llegue', 'al', '[MASK]', 'ca', '[MASK]', '[MASK]', 'que', 'el', '[MASK]', '[MASK]', 'de', 'la', 'loco', '[MASK]', '[MASK]', 'o', 'ha', 'manifestado', 'que', 'él', '[MASK]', 'lo', 'personal', 'no', 'está', 'teme', 'ros', '##o', 'del', 'problema', '[MASK]', 'porque', 'c', 'en', 'a', 'con', 'le', '[MASK]', 'da', 'del', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'arce', '[MASK]', '[MASK]', '2', '[MASK]', 'o', '[MASK]', '[MASK]', '9', '##6', '##7', 'se', 'tiene', '[MASK]', '[MASK]', 'de', 'que', '[MASK]', '[MASK]', 'a', 'h', 'de', 'los', 'corrientes', 'el', 'dio', '[MASK]', '[MASK]', 'del', '[MASK]', 'gel', 'fuentes', '[MASK]', '[MASK]', 'salir', 'con', 'destino', 'a', 'o', '[MASK]', '[MASK]', 'con', '[MASK]', 'objeto', 'de', '[MASK]', '[MASK]', 'ta', '[MASK]', '[MASK]', 'ea', 'd', 's', 'ma', 'c', '[UNK]', 'na', '[MASK]', '[UNK]', 'en', 'o', 'a', '[UNK]', 'a', '[SEP]']\n",
            "Sugerencias SpellChecker ['dórico']\n",
            "Sugerencias BERT ['basado', 'basa', 'que', 'tiene', 'esta']\n",
            "Sugerencias SpellChecker ['tomando', 'tomento', 'tomen do', 'tomen-do', 'momento']\n",
            "Sugerencias BERT ['gobierno', 'tribunal', 'el', 'de', 'que']\n",
            "Sugerencias SpellChecker ['Mue', 'Fue', 'Que', 'Je', 'Jure', 'Juez']\n",
            "Sugerencias BERT ['de', 'para', 'define', 'a', 'regula']\n",
            "Sugerencias SpellChecker ['lignificó']\n",
            "Sugerencias BERT ['convocar', 'que', 'va', 'a', 'no']\n",
            "Sugerencias SpellChecker ['convoque', 'convoqué', 'convolución']\n",
            "Sugerencias BERT ['de', 'en', 'a', 'sobre', 'para']\n",
            "Sugerencias SpellChecker ['en', 'eón', 'ene', 'eren', 'leen', 'meen', 'peen', 'sen', 'len', 'ten', 'den', 'gen', 'zen', 'ven', 'yen']\n",
            "Sugerencias BERT ['d', 'e', 'de', '.', 'a']\n",
            "Sugerencias SpellChecker ['Noé', 'No', 'Nos', 'Non', 'Con', 'Oc', 'Noca']\n",
            "Sugerencias BERT ['d', 'se', 'e', ',', 'c']\n",
            "Sugerencias SpellChecker ['Meé', 'Se', 'Sé', 'Aseé', 'Seré', 'Oseé', 'Sedé', 'Ése', 'Sea', 'Ser', 'Ses', 'Seo', 'Sen', 'Sed', 'Seó']\n",
            "Sugerencias BERT ['que', 'se', 'el', 'no', 'pe']\n",
            "Sugerencias SpellChecker ['correr', 'correo', 'correros', 'correrlo', 'corroer', 'carrero', 'cornero', 'torrero', 'cordero', 'correrá', 'gorrero', 'zorrero', 'cofrero', 'horrero', 'correré']\n",
            "Sugerencias BERT ['el', 'del', 'gobierno', 'de', 'en']\n",
            "Sugerencias SpellChecker ['dirimente', 'dirigente', 'iridiscente', 'dicente', 'discente', 'diciente']\n",
            "Sugerencias BERT ['de', 'es', 'el', 'e', 'presidente']\n",
            "Sugerencias SpellChecker ['máximo', 'máximum']\n",
            "Sugerencias BERT ['empresa', 'persona', 'es', 'para', ',']\n",
            "Sugerencias SpellChecker ['Central', 'Centrar']\n",
            "Sugerencias BERT ['el', 'de', 'a', 'del', 'al']\n",
            "Sugerencias SpellChecker ['Congo', 'Gong', 'Con', 'Conga', 'Cono']\n",
            "Sugerencias BERT ['es', 'el', 'de', 'a', ',']\n",
            "Sugerencias SpellChecker ['a', 'ea', 'na', 'la', 'ta', 'ca', 'da', 'fa', 'va', 'ha', 'ja', 'ya', 'ka']\n",
            "Sugerencias BERT ['de', ',', 'y', 'en', 'el']\n",
            "Sugerencias SpellChecker ['Unión']\n",
            "Sugerencias BERT ['el', 'presidente', 'del', 'gobierno', 'dia']\n",
            "Sugerencias SpellChecker ['Ario', 'Siro', 'Serio', 'Sirio', 'Riso']\n",
            "Sugerencias BERT ['presidente', 'ejecutivo', 'gobierno', 'el', 'dia']\n",
            "Sugerencias SpellChecker ['Eral', 'Oral', 'Cal', 'Coral', 'Cual', 'Chal', 'Craneal']\n",
            "Sugerencias BERT ['de', 'es', 'el', 'que', 'en']\n",
            "Sugerencias SpellChecker ['pode', 'jode']\n",
            "Sugerencias BERT ['el', '[UNK]', 'de', 'que', 'es']\n",
            "Sugerencias SpellChecker ['Gro', 'Grao', 'Roo', 'Groso', 'Ogro', 'Gros', 'Croo', 'Gro o']\n",
            "Sugerencias BERT ['de', 'en', 'el', 'es', 'a']\n",
            "Sugerencias SpellChecker ['na', 'a', 'san', 'ano', 'tan', 'can', 'dan', 'aun', 'pan', 'fan', 'van', 'han', 'aún', 'kan', 'en']\n",
            "Sugerencias BERT [',', 'de', 'el', 'y', '.']\n",
            "Sugerencias SpellChecker ['coplero', 'cofrero', 'coopero']\n",
            "Sugerencias BERT ['la', 'a', 'el', 'le', 'da']\n",
            "Sugerencias SpellChecker ['ay', 'al', 'lay', 'aya', 'ayo']\n",
            "Sugerencias BERT ['del', 'de', 'el', 'dia', '[UNK]']\n",
            "Sugerencias SpellChecker ['Dio', 'Di']\n",
            "Sugerencias BERT ['de', 'del', 'el', '[UNK]', 'e']\n",
            "Sugerencias SpellChecker ['Oc', 'Col', 'Lo', 'Loca', 'Loco', 'Bloc', 'Loa', 'Loe', 'Los', 'Loo', 'Loé', 'Loó']\n",
            "Sugerencias BERT ['de', 'el', '[UNK]', 'del', 'e']\n",
            "Sugerencias SpellChecker ['CLIC', 'LIS', 'SIC', 'TIC', 'LID', 'LIÉ', 'LIÓ']\n",
            "Sugerencias BERT ['de', 'el', '[UNK]', 'del', 'es']\n",
            "Sugerencias SpellChecker ['RUMBEN', 'SUBEN', 'ROBEN', 'RULEN', 'RUTEN', 'RUGEN', 'RUBÍN', 'RUMBEEN']\n",
            "Sugerencias BERT ['de', 'el', 'del', '[UNK]', 'es']\n",
            "Sugerencias SpellChecker ['ZUMO', 'UNO', 'UNZO', 'ZURO', 'RUNO', 'ZULO', 'TUNO', 'CUNO', 'PUNO', 'HUNO', 'ZUÑO', 'ZUNCHO']\n",
            "Sugerencias BERT ['[UNK]', '1', '9', '(', 't']\n",
            "Sugerencias SpellChecker ['píxel']\n",
            "Sugerencias BERT ['[UNK]', '1', '(', 'es', 't']\n",
            "Sugerencias SpellChecker []\n",
            "Sugerencias BERT ['[UNK]', 'd', '1', '\"', 't']\n",
            "Sugerencias SpellChecker ['la', 'le', 'lo', 'algo']\n",
            "Sugerencias BERT ['[UNK]', '1', '9', 'n', '3']\n",
            "Sugerencias SpellChecker ['Ah', 'A', 'Gag', 'As', 'Al', 'Ay']\n",
            "Sugerencias BERT ['[UNK]', '1', '(', '-', 'o']\n",
            "Sugerencias SpellChecker ['toa', 'toe', 'tos', 'too', 'ton', 'toé', 'toó']\n",
            "Sugerencias BERT ['el', 'la', 'ni', 'da', 'del']\n",
            "Sugerencias SpellChecker ['conciencie']\n",
            "Sugerencias BERT ['cuenta', 'idea', 'noticia', 'de', 'conocimiento']\n",
            "Sugerencias SpellChecker ['no', 'nato', 'neto', 'noto', 'nito', 'unto', 'ton', 'nao', 'ato', 'neo']\n",
            "Sugerencias BERT ['el', 'la', 'a', '[UNK]', 'de']\n",
            "Sugerencias SpellChecker ['cal', 'col', 'ca', 'al', 'ce', 'el', 'cm', 'él', 'clic']\n",
            "Sugerencias BERT ['a', 'de', '[UNK]', 'el', 'd']\n",
            "Sugerencias SpellChecker ['diada']\n",
            "Sugerencias BERT ['el', 'de', 'del', 'a', 'al']\n",
            "Sugerencias SpellChecker ['Efod']\n",
            "Sugerencias BERT ['el', 'es', 'a', 'd', 'e']\n",
            "Sugerencias SpellChecker ['Curas', 'Usar', 'Causar', 'Acusar', 'Cursar', 'Cursa', 'Casar', 'Cesar', 'Curar', 'Cunar', 'Cular', 'Cucar', 'César', 'Cuñar', 'Sacuara']\n",
            "Sugerencias BERT ['el', 'del', 'de', 'gobierno', 'dia']\n",
            "Sugerencias SpellChecker ['Allozo', 'Al loo', 'Al-loo']\n",
            "Sugerencias BERT ['que', 'de', 'se', 'para', 'puede']\n",
            "Sugerencias SpellChecker ['nieto', 'tino', 'tierno', 'tiento', 'tiendo', 'tiene', 'treno', 'tieso', 'cieno', 'tizno', 'tirreno']\n",
            "Sugerencias BERT ['para', 'puede', 'que', 'de', 'a']\n",
            "Sugerencias SpellChecker ['posado', 'posando', 'pensado', 'responsado']\n",
            "Sugerencias BERT ['a', 'de', 'd', 'o', '[UNK]']\n",
            "Sugerencias SpellChecker ['Acapullaos', 'Acapullaros', 'Acapullas', 'Capullos']\n",
            "Sugerencias BERT ['con', ',', 'a', 'd', 'de']\n",
            "Sugerencias SpellChecker ['Groso']\n",
            "Sugerencias BERT ['el', 'ei', 'un', 'con', 'n']\n",
            "Sugerencias SpellChecker ['cal', 'col', 'ca', 'al', 'ce', 'el', 'cm', 'él', 'clic']\n",
            "Sugerencias BERT ['de', 'hacer', 'la', 'crear', 'que']\n",
            "Sugerencias SpellChecker ['conseguir']\n",
            "Sugerencias BERT ['a', 'la', 'el', '[UNK]', 'de']\n",
            "Sugerencias SpellChecker ['mandilando']\n",
            "Sugerencias BERT ['a', '[UNK]', 'd', 'de', 'la']\n",
            "Sugerencias SpellChecker ['E', 'ERE', 'ESE', 'ENE', 'ELE', 'LEE', 'EME', 'MEE', 'PEE', 'EFE', 'EJE', 'EÑE', 'EA', 'ES', 'SE']\n",
            "Sugerencias BERT ['a', '[UNK]', 'de', 'e', 'la']\n",
            "Sugerencias SpellChecker ['NAS', 'NA', 'NASA', 'NANA', 'NATA', 'NADA', 'NABA', 'NAPA', 'NAVA', 'NAJA', 'NAO', 'NÍA', 'NA A']\n",
            "Sugerencias BERT ['a', '[UNK]', 'd', 't', 'de']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:15<00:00, 15.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======================Texto BERT======================\n",
            "[CLS] dgric  y basado en consideración el fallo del gobierno   que de las elecciones en cuestión  declare   acéfala la actual directiva de la Unión y no a nuevos Comicios    sobre la C  de    c hacen comentarios en el sentido de que el descontento se  es utilizado en contra del gobierno presidente de   esa empresa  para que  éste no llegue al el  Ca   a y que el gobierno  gobierno  de la Loco  el de   o ha manifestado que él  a lo personal  no está teme roso del problema de  porque c en a con le le da del el    e  [UNK] ARCE  [UNK]  es      2  o   967   Se tiene el conocimiento de que a el a h de los corrientes   el Dio   a       d DEL gobierno GEL FUENTES    puede salir con destino a   o a con con con objeto de hacer el  TA la [UNK] EA D s   MA c — NA [UNK] — en o a   — a \f [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}